{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a253932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import Any\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "from evidently import Dataset\n",
    "from evidently import DataDefinition\n",
    "from evidently import Report\n",
    "from evidently.metrics import *\n",
    "from evidently.presets import *\n",
    "from evidently.tests import *\n",
    "import logging\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from evidently import MulticlassClassification\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging():\n",
    "    \"\"\"Configure logging handlers and return a logger instance.\"\"\"\n",
    "    if Path(\"logging.conf\").exists():\n",
    "        logging.config.fileConfig(\"logging.conf\")\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "            handlers=[logging.StreamHandler(sys.stdout)],\n",
    "            level=logging.INFO,\n",
    "        )\n",
    "\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24698b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_drift() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a random drift to the body mass measurements in the penguins dataset.\n",
    "\n",
    "    This function reads the `data/penguins.csv` file into a pandas DataFrame,\n",
    "    removes the 'species' column, and introduces a stochastic drift to the\n",
    "    'body_mass_g' column. The drift is uniformly distributed between 1 and\n",
    "    three times the standard deviation of the original 'body_mass_g' values.\n",
    "    The modified DataFrame is then returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the penguins dataset with the 'species' column\n",
    "        removed and the 'body_mass_g' values perturbed by a random drift.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function assumes that `data/penguins.csv` exists and contains a\n",
    "      'body_mass_g' column.\n",
    "    - The random number generator used is `numpy.random.default_rng()`, ensuring\n",
    "      reproducibility control if desired.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the original dataset from the data direcory in the projects root\n",
    "    data = pd.read_csv(\"data/penguins.csv\")\n",
    "    # remove the ground truth label\n",
    "    data.pop('species')\n",
    "    # instantiate a uniformly distributed randon number generate\n",
    "    rng = np.random.default_rng()\n",
    "    # add a random number between 1 and 3 times its own std to body_mass_g,\n",
    "    # flipper_length_mm, bill_depth_mm and bill_length_mm\n",
    "    data['body_mass_g'] += rng.uniform(\n",
    "        1,\n",
    "        3 * data[\"body_mass_g\"].std(),\n",
    "        size=len(data)\n",
    "    )\n",
    "\n",
    "    data[\"flipper_length_mm\"] += rng.uniform(\n",
    "        1,\n",
    "        3 * data[\"flipper_length_mm\"].std(),\n",
    "        size=len(data)\n",
    "    )\n",
    "\n",
    "    data[\"bill_depth_mm\"] += rng.uniform(\n",
    "        1,\n",
    "        3 * data[\"bill_depth_mm\"].std(),\n",
    "    )\n",
    "    \n",
    "    data[\"bill_length_mm\"] += rng.uniform(\n",
    "        1,\n",
    "        3 * data[\"bill_length_mm\"].std(),\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_features = apply_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62559129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifications(payload: dict[str, Any]) \\\n",
    "        -> dict[str, Any] | list[Any] | None:\n",
    "    \"\"\"\n",
    "    Send a classification request to the inference endpoint and return the results.\n",
    "\n",
    "    This function makes a POST request to a local inference endpoint with the\n",
    "    provided payload and returns the classification results if successful.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    payload : dict[str, Any]\n",
    "        The input data to be classified. Should contain an 'inputs' key with\n",
    "        the data formatted according to the endpoint's expected schema.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any] or list[Any] or None\n",
    "        The classification results returned by the inference endpoint if the\n",
    "        request is successful (status code 200). Returns None if the request\n",
    "        fails or encounters an error.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function sends requests to http://0.0.0.0:8080/invocations\n",
    "    - Uses JSON content type for the request\n",
    "    - Logs error information if the response status code is not 200\n",
    "    - Does not raise exceptions; returns None on failure\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    payload = {\"inputs\": [{\"feature1\": 1.0, \"feature2\": 2.0}]}\n",
    "    result = generate_classifications(payload)\n",
    "    print(result)\n",
    "    {'predictions': [0.85, 0.15]}\n",
    "\n",
    "    invalid_payload = {}\n",
    "    result = generate_classifications(invalid_payload)\n",
    "    Error: 400, text: Invalid input format\n",
    "    print(result)\n",
    "    None\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    generate_traffic : Uses this function to generate classifications in batches\n",
    "    \"\"\"\n",
    "    # locally hosted model URL\n",
    "    url = \"http://0.0.0.0:8080/invocations\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        logging.info(\"Error: %s, text: s%\", response.status_code, response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ac959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_traffic(model_input: pd.DataFrame,\n",
    "                    classifications: dict, data_uri: str) -> None:\n",
    "    \"\"\"\n",
    "    Store model input data and predictions in a SQLite database.\n",
    "\n",
    "    This function captures inference traffic by saving the input features,\n",
    "    predictions, and metadata (timestamps and UUIDs) to a SQLite database\n",
    "    for monitoring and analysis purposes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_input : pd.DataFrame\n",
    "    The input features used for model inference. This DataFrame is copied\n",
    "    and augmented with additional columns before storage.\n",
    "    classifications : list\n",
    "        The classification results from the model. Expected to be a dictionary-like\n",
    "        object with a 'predictions' key containing a list of prediction values.\n",
    "        If None or empty, the classification column will contain None values.\n",
    "    data_uri : str\n",
    "        The file path or URI to the SQLite database where the data will be stored.\n",
    "        The data is appended to a table named 'data'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return a value. Data is stored directly in the\n",
    "        specified database.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Creates a copy of the input DataFrame to avoid modifying the original\n",
    "    - Adds the following columns to the data before storage:\n",
    "        * date: Current UTC timestamp\n",
    "        * classification: Model predictions from the classifications parameter\n",
    "        * ground_truth: Initialized as None (for later labeling)\n",
    "        * uuid: Unique identifier for each row (UUID4 format)\n",
    "    - Appends data to the 'data' table in the SQLite database\n",
    "    - Logs errors if database operations fail but does not raise exceptions\n",
    "    - Ensures the database connection is properly closed in all cases\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    import pandas as pd\n",
    "    model_input = pd.DataFrame({'feature1': [1.0, 2.0], 'feature2': [3.0, 4.0]})\n",
    "    classifications = {'predictions': [0, 1]}\n",
    "    capture_traffic(model_input, classifications, 'inference_data.db')\n",
    "    # Stores the input and predictions in inference_data.db\n",
    "\n",
    "    # Handle case with no predictions\n",
    "    capture_traffic(model_input, None, 'inference_data.db')\n",
    "    # Stores input with None values for classifications\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    generate_traffic : Uses this function to store batches of inference data\n",
    "    \"\"\"\n",
    "    logging.info(\"Storing input payload and predictions in the database...\")\n",
    "    connection = None\n",
    "    try:\n",
    "        # connect to the database\n",
    "        connection = sqlite3.connect(data_uri)\n",
    "        # assign input data to variable 'data'\n",
    "        data = model_input.copy()\n",
    "        # add date column to DatFrame\n",
    "        data[\"date\"] = datetime.now(timezone.utc)\n",
    "        #add classification column to DataFrame\n",
    "        data[\"classification\"] = None\n",
    "        # add ground_truth column to DataFrame\n",
    "        data[\"ground_truth\"] = None\n",
    "        # add model classification to drift DataFrame\n",
    "        if classifications is not None and len(classifications) > 0:\n",
    "            data[\"classification\"] = [item['classification'] for item in classifications['predictions']]\n",
    "\n",
    "        # add unique id to the DataFrame\n",
    "        data[\"uuid\"] = [str(uuid.uuid4()) for _ in range(len(data))]\n",
    "        # save DataFrame to database\n",
    "        data.to_sql(\"data\", connection, if_exists=\"append\", index=False)\n",
    "\n",
    "    except sqlite3.Error:\n",
    "        logging.exception(\n",
    "            \"There was an error saving the input request and output prediction \"\n",
    "            \"in the database.\",\n",
    "        )\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traffic(data: pd.DataFrame, samples: int) -> None:\n",
    "    \"\"\"\n",
    "    Generate synthetic traffic by sampling data and creating classifications.\n",
    "\n",
    "    This function repeatedly samples batches of data, generates classifications\n",
    "    for each batch, and stores the results in a SQLite database. It continues\n",
    "    until the specified number of samples have been processed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The source DataFrame from which to sample data for classification.\n",
    "        Must contain the required features for generating classifications.\n",
    "    samples : int\n",
    "        The target number of samples to generate. The function will continue\n",
    "        processing batches until at least this many classifications have been\n",
    "        created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return a value. Results are stored in the\n",
    "        'inference_data.db' SQLite database.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Samples data in batches of 10 rows\n",
    "    - Replaces NaN, inf, and -inf values with None before processing\n",
    "    - Stores classification results in 'inference_data.db' via the\n",
    "        capture_traffic function\n",
    "    - Logs progress and errors throughout execution\n",
    "    - Queries and logs the last 5 database entries upon completion\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n",
    "    generate_traffic(df, samples=100)\n",
    "    # Generates 100 classifications and stores them in the database\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    generate_classifications : Generates classifications from input payload\n",
    "    capture_traffic : Stores batch data and classifications in the database\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        classifications_generated = 0\n",
    "        # send data to the endpoint while classifications are less than classifications\n",
    "        # generated by the model\n",
    "        while classifications_generated < samples:\n",
    "            # instantiate dictionary called payload\n",
    "            payload = {}\n",
    "            #sample from the drift dataset replacing NaN values with none to ensure\n",
    "            #compatibility with JSON objects\n",
    "            batch = data.sample(n=10).replace([np.nan, np.inf, -np.inf], None)\n",
    "            # generate a dictionary of inputs to send to the enpoint\n",
    "            payload[\"inputs\"] = [\n",
    "                {\n",
    "                    k: (None if pd.isna(v) else v)\n",
    "                    for k, v in row.to_dict().items()\n",
    "                }\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "            # send inputs to the endpoint\n",
    "            classifications = generate_classifications(payload)\n",
    "            logging.info(f\"Generated classifications %s\", classifications)\n",
    "            try:\n",
    "                # capture the input data and classifications and save it to the database\n",
    "                capture_traffic(batch, classifications,\"inference_data.db\")\n",
    "            except sqlite3.Error:\n",
    "                logging.exception(\n",
    "                    \"There was an error connecting to the database. \",\n",
    "                )\n",
    "            # update the count of classifications generated\n",
    "            classifications_generated += len(batch)\n",
    "            logging.info(\"Generated %s classifications\",classifications_generated)\n",
    "\n",
    "    except Exception:\n",
    "        logging.exception(\"There was an error sending traffic to the endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_traffic(drift_features, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f026ed",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d53435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve all unlabeled data from the inference database.\n",
    "\n",
    "    Connects to the SQLite database, executes a query to fetch all records\n",
    "    from the data table where ground_truth is NULL (unlabeled data), and\n",
    "    returns the results as a pandas DataFrame. Records are ordered by date\n",
    "    in descending order (newest first).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing all unlabeled records with the following columns:\n",
    "        - uuid : unique identifier for each record\n",
    "        - island : island location\n",
    "        - sex : sex of the penguin\n",
    "        - bill_length_mm : bill length in millimeters\n",
    "        - bill_depth_mm : bill depth in millimeters\n",
    "        - flipper_length_mm : flipper length in millimeters\n",
    "        - body_mass_g : body mass in grams\n",
    "        - classification : model prediction\n",
    "        - ground_truth : actual classification label (NULL for unlabeled data)\n",
    "\n",
    "        Records are sorted by date in descending order.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function establishes a connection to 'inference_data.db', retrieves\n",
    "    only data where ground_truth is NULL (hasn't been labeled yet), and\n",
    "    properly closes the connection before returning.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    data = retrieve_data()\n",
    "    print(data.head())\n",
    "    print(data['ground_truth'].isna().all())  # Should return True\n",
    "    \"\"\"\n",
    "    # Establish connection to the SQLite database\n",
    "    connection = sqlite3.connect(\"inference_data.db\")\n",
    "\n",
    "    logging.info(\"Retrieving data....\")\n",
    "\n",
    "    # SQL query to select all relevant columns from the data table\n",
    "    # Only retrieve records where ground_truth is NULL (unlabeled)\n",
    "    # Results are ordered by date in descending order (newest first)\n",
    "    query = (\n",
    "        \"SELECT uuid, island, sex, bill_length_mm, bill_depth_mm, flipper_length_mm, \"\n",
    "        \"body_mass_g, classification, ground_truth FROM data \"\n",
    "        \"WHERE ground_truth IS NULL \"\n",
    "        \"ORDER BY date DESC;\"\n",
    "    )\n",
    "\n",
    "    # Execute query and load results into a pandas DataFrame\n",
    "    data = pd.read_sql_query(query, connection)\n",
    "\n",
    "    logging.info(\"Retrieved data\")\n",
    "\n",
    "    # Close the database connection\n",
    "    connection.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42386ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prediction: str, prediction_quality: float) -> str:\n",
    "    \"\"\"\n",
    "    Generate a simulated ground truth label based on prediction quality.\n",
    "\n",
    "    This function creates synthetic ground truth labels for testing and simulation\n",
    "    purposes. It returns the predicted label with a probability equal to the\n",
    "    prediction quality, otherwise returns a random label from the possible classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : str\n",
    "        The model's predicted label. Should be one of the valid penguin species:\n",
    "        \"Adelie\", \"Chinstrap\", or \"Gentoo\".\n",
    "    prediction_quality : float\n",
    "        The desired accuracy rate for the predictions, expressed as a probability\n",
    "        between 0.0 and 1.0. A value of 0.8 means the ground truth will match\n",
    "        the prediction 80% of the time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A ground truth label. Returns the input prediction with probability equal\n",
    "        to `prediction_quality`, otherwise returns a randomly selected label from\n",
    "        [\"Adelie\", \"Chinstrap\", \"Gentoo\"].\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function is intended for simulation and testing purposes only\n",
    "    - Uses random selection, so results are non-deterministic\n",
    "    - Does not validate that the prediction is a valid penguin species\n",
    "    - The random label may coincidentally match the prediction even when\n",
    "      the quality check fails\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    # With high prediction quality, usually returns the prediction\n",
    "    label = get_label(\"Adelie\", prediction_quality=0.9)\n",
    "    # Returns \"Adelie\" 90% of the time, random species 10% of the time\n",
    "\n",
    "    # With low prediction quality, often returns a different label\n",
    "    label = get_label(\"Chinstrap\", prediction_quality=0.3)\n",
    "    # Returns \"Chinstrap\" 30% of the time, random species 70% of the time\n",
    "\n",
    "    # Simulate perfect predictions\n",
    "    label = get_label(\"Gentoo\", prediction_quality=1.0)\n",
    "    print(label)\n",
    "    'Gentoo'\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    random.choice : Used to select random labels when prediction quality check fails\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        prediction\n",
    "        if random.random() < prediction_quality\n",
    "        else random.choice([\"Adelie\", \"Chinstrap\", \"Gentoo\"])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3295018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(data: pd.DataFrame) -> int|None:\n",
    "    \"\"\"\n",
    "    Generate and store simulated ground truth labels for inference data.\n",
    "\n",
    "    This function creates synthetic ground truth labels for the provided data\n",
    "    and updates the corresponding records in the database. Labels are generated\n",
    "    with a simulated accuracy of 80% using the get_label function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        A DataFrame containing inference records to label. Must include 'uuid'\n",
    "        and 'classification' columns. The 'uuid' column is used to identify\n",
    "        records in the database for updating.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None or int\n",
    "        Returns 0 if the input DataFrame is empty, otherwise returns None after\n",
    "        successfully updating the database with ground truth labels.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Connects to 'inference_data.db' SQLite database\n",
    "    - Updates the 'ground_truth' column in the 'data' table\n",
    "    - Uses a fixed prediction quality of 0.8 (80% accuracy) for label generation\n",
    "    - Commits all updates in a single transaction after processing all rows\n",
    "    - Logs the labeling process for monitoring purposes\n",
    "    - Automatically closes the database connection after updates\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    import pandas as pd\n",
    "    data = pd.DataFrame({\n",
    "    ...     'uuid': ['123e4567-e89b-12d3-a456-426614174000'],\n",
    "    ...     'classification': ['Adelie']\n",
    "    ... })\n",
    "    label_data(data)\n",
    "    # Updates the database with ground truth labels\n",
    "\n",
    "    # Handle empty DataFrame\n",
    "    empty_data = pd.DataFrame()\n",
    "    result = label_data(empty_data)\n",
    "    print(result)\n",
    "    0\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    get_label : Generates individual ground truth labels based on predictions\n",
    "    retrieve_data : Retrieves inference data from the database for labeling\n",
    "    \"\"\"\n",
    "\n",
    "    # Establish connection to the SQLite database\n",
    "    connection = sqlite3.connect(\"inference_data.db\")\n",
    "\n",
    "    # Check if the input DataFrame is empty and return early if so\n",
    "    if data.empty:\n",
    "        return 0\n",
    "\n",
    "    logging.info(\"Generating ground truth labels...\")\n",
    "\n",
    "    # Iterate through each row in the DataFrame to generate and store labels\n",
    "    for _, row in data.iterrows():\n",
    "        # Extract the unique identifier for the current record\n",
    "        uuid = row[\"uuid\"]\n",
    "\n",
    "        # Generate a simulated ground truth label with 80% accuracy\n",
    "        # based on the model's classification\n",
    "        label = get_label(row[\"classification\"], 0.8)\n",
    "\n",
    "        # Prepare SQL query to update the ground_truth column for this record\n",
    "        update_query = \"UPDATE data SET ground_truth = ? WHERE uuid = ?\"\n",
    "\n",
    "        # Execute the update query with the generated label and uuid\n",
    "        connection.execute(update_query, (label, uuid))\n",
    "\n",
    "    # Commit all updates to the database in a single transaction\n",
    "    connection.commit()\n",
    "\n",
    "    # Close the database connection to free resources\n",
    "    connection.close()\n",
    "\n",
    "    logging.info(\"Generated ground truth labels\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_inference_data = retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data(drift_inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845ad2b",
   "metadata": {},
   "source": [
    "#Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_labelled_data(samples: int) -> pd.DataFrame:\n",
    "    connection = sqlite3.connect(\"inference_data.db\")\n",
    "    logging.info(\"Retrieving data....\")\n",
    "\n",
    "    query = (\n",
    "                \"SELECT island, sex, bill_length_mm, bill_depth_mm, flipper_length_mm, \"\n",
    "                \"body_mass_g, classification, ground_truth FROM data \"\n",
    "                \"ORDER BY date DESC LIMIT ?;\"\n",
    "            )\n",
    "    data = pd.read_sql_query(query, connection, params=(samples,))\n",
    "    logging.info(\"Retrieved data\")\n",
    "    connection.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets() -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Create reference and current Evidently AI datasets for model monitoring.\n",
    "\n",
    "    This function prepares two datasets for monitoring: a reference dataset from\n",
    "    historical penguin data and a current dataset from recent inference results.\n",
    "    Both datasets are configured with the same schema defining numerical and\n",
    "    categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    evidently_training_dataset : Dataset\n",
    "        The reference Evidently AI dataset created from the historical penguin\n",
    "        data (penguins.csv). The 'species' column is renamed to 'ground_truth'\n",
    "        and duplicated as 'classification' to match the monitoring schema.\n",
    "    evidently_inference_dataset : Dataset\n",
    "        The current Evidently AI dataset created from the 100 most recent\n",
    "        inference records retrieved from the database.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Defines a schema with the following structure:\n",
    "        * Numerical columns: bill_length_mm, bill_depth_mm, flipper_length_mm,\n",
    "          body_mass_g\n",
    "        * Categorical columns: classification, ground_truth, island, sex\n",
    "    - Reference data is loaded from 'data/penguins.csv'\n",
    "    - Current data retrieves the 100 most recent records from the database\n",
    "    - Reference dataset has 'species' renamed to 'ground_truth' and copied to\n",
    "      'classification' for consistency with inference data format\n",
    "    - Both datasets use the same data definition schema for comparability\n",
    "    - Logs the dataset creation process\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    ref_dataset, curr_dataset = create_datasets()\n",
    "    print(ref_dataset.data.columns.tolist())\n",
    "    ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g',\n",
    "     'classification', 'ground_truth', 'island', 'sex']\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    retrieve_data : Retrieves current inference data from the database\n",
    "    Dataset.from_pandas : Creates Evidently AI datasets from pandas DataFrames\n",
    "    DataDefinition : Defines the schema for Evidently AI datasets\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating datasets\")\n",
    "    # Create a monitoring schema for the dataset\n",
    "    schema = DataDefinition(\n",
    "        numerical_columns=[\n",
    "            \"bill_length_mm\",\n",
    "            \"bill_depth_mm\",\n",
    "            \"flipper_length_mm\",\n",
    "            \"body_mass_g\",\n",
    "        ],\n",
    "        categorical_columns=[\n",
    "            \"classification\",\n",
    "            \"ground_truth\",\n",
    "            \"island\",\n",
    "            \"sex\",\n",
    "        ],\n",
    "        classification=[MulticlassClassification(\n",
    "            target=\"ground_truth\",\n",
    "            prediction_labels=\"classification\",\n",
    "        )]\n",
    "    )\n",
    "    # create a DataFrame with historical data\n",
    "    training_data = pd.read_csv(\"data/penguins.csv\")\n",
    "    # rename the species column as ground_truth for consistency with inference\n",
    "    # data\n",
    "    training_data.rename(columns={\"species\":\"ground_truth\"}, inplace=True)\n",
    "    # create a classification column to match monitoring schema\n",
    "    training_data[\"classification\"] = training_data[\"ground_truth\"]\n",
    "    # retrieve inference data from database\n",
    "    inference_data = retrieve_labelled_data(200)\n",
    "    # create dataset object with the historical data\n",
    "    evidently_training_dataset = Dataset.from_pandas(\n",
    "        training_data,\n",
    "        data_definition=schema\n",
    "    )\n",
    "    # Create a dataset object with the inference data\n",
    "    evidently_inference_dataset = Dataset.from_pandas(\n",
    "        inference_data,\n",
    "        data_definition=schema\n",
    "    )\n",
    "\n",
    "    # return a tuple of the historical and inference dataset objects\n",
    "    return evidently_training_dataset, evidently_inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_training_dataset, evidently_inference_dataset = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_report(training_dataset: Dataset, inference_dataset: Dataset) -> Report:\n",
    "    \"\"\"\n",
    "    Run column-level statistical reports on selected features.\n",
    "\n",
    "    This function computes column-specific metrics comparing inference data\n",
    "    against a training (reference) dataset using Evidently AI metric primitives.\n",
    "    The report focuses on median values for numerical columns and unique value\n",
    "    counts for categorical columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dataset : Dataset\n",
    "        The reference Evidently AI dataset, typically derived from training\n",
    "        or historical data, used as the baseline for comparison.\n",
    "    inference_dataset : Dataset\n",
    "        The current Evidently AI dataset containing recent inference data\n",
    "        to be evaluated against the reference dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    EvaluationResult\n",
    "        The result of running the Evidently AI report, containing computed\n",
    "        column-level statistics and comparison metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Computes median values for numerical columns:\n",
    "        * bill_length_mm\n",
    "        * bill_depth_mm\n",
    "        * flipper_length_mm\n",
    "        * body_mass_g\n",
    "    - Computes unique value counts for categorical columns:\n",
    "        * island\n",
    "        * sex\n",
    "    - Compares inference data against training data\n",
    "    - Returns the executed report results to the caller\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    train_data, infer_data = create_datasets()\n",
    "    eval_result = column_report(train_data, infer_data)\n",
    "    # Inspect column-level metrics in eval_result\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Report : Evidently AI report class for generating analyses\n",
    "    MedianValue : Metric for computing median values of numeric columns\n",
    "    UniqueValueCount : Metric for counting unique values in a column\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    column_report = Report([\n",
    "        MedianValue(column=\"bill_length_mm\"),\n",
    "        MedianValue(column=\"bill_depth_mm\"),\n",
    "        MedianValue(column=\"flipper_length_mm\"),\n",
    "        MedianValue(column=\"body_mass_g\"),\n",
    "        UniqueValueCount(column=\"island\"),\n",
    "        UniqueValueCount(column=\"sex\"),\n",
    "    ])\n",
    "    \n",
    "    # Execute the evaluations\n",
    "    column_report = column_report.run(inference_dataset, training_dataset)\n",
    "    \n",
    "    return column_report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_report = column_report(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "column_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_report(training_dataset: Dataset, inference_dataset: Dataset) -> Report:\n",
    "    \"\"\"\n",
    "    Run dataset-level summary statistics reports.\n",
    "\n",
    "    This function computes high-level dataset metrics for both training\n",
    "    (reference) and inference datasets using Evidently AI dataset-level\n",
    "    metrics. The report captures basic structural and completeness\n",
    "    characteristics of the datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dataset : Dataset\n",
    "        The reference Evidently AI dataset, typically derived from training\n",
    "        or historical data.\n",
    "    inference_dataset : Dataset\n",
    "        The current Evidently AI dataset containing recent inference data\n",
    "        to be evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    EvaluationResult\n",
    "        The result of running the Evidently AI report, containing computed\n",
    "        dataset-level statistics and metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Computes the following dataset-level metrics:\n",
    "        * Row count\n",
    "        * Column count\n",
    "        * Number of empty columns\n",
    "        * Number of empty rows\n",
    "    - Evaluates metrics for both training and inference datasets\n",
    "    - Returns the executed report results to the caller\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    train_data, infer_data = create_datasets()\n",
    "    eval_result = dataset_report(train_data, infer_data)\n",
    "    # Inspect dataset-level metrics in eval_result\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Report : Evidently AI report class for generating analyses\n",
    "    RowCount : Metric for counting dataset rows\n",
    "    ColumnCount : Metric for counting dataset columns\n",
    "    EmptyColumnsCount : Metric for counting empty columns\n",
    "    EmptyRowsCount : Metric for counting empty rows\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_report = Report([\n",
    "        RowCount(),\n",
    "        ColumnCount(),\n",
    "        EmptyColumnsCount(),\n",
    "        EmptyRowsCount(),  \n",
    "    ])\n",
    "\n",
    "    dataset_report = dataset_report.run(training_dataset, inference_dataset)\n",
    "    \n",
    "    return dataset_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b52cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_report = dataset_report(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "dataset_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_report_with_tests(training_dataset: Dataset, inference_dataset: Dataset) -> Report:\n",
    "    \"\"\"\n",
    "    Run column-level statistical reports with automated tests enabled.\n",
    "\n",
    "    This function computes column-specific metrics comparing inference data\n",
    "    against a training (reference) dataset using Evidently AI metric primitives,\n",
    "    and includes built-in statistical tests for detecting significant changes\n",
    "    between datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dataset : Dataset\n",
    "        The reference Evidently AI dataset, typically derived from training\n",
    "        or historical data, used as the baseline for comparison.\n",
    "    inference_dataset : Dataset\n",
    "        The current Evidently AI dataset containing recent inference data\n",
    "        to be evaluated against the reference dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    EvaluationResult\n",
    "        The result of running the Evidently AI report, containing computed\n",
    "        column-level statistics and associated test results.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Computes median values for numerical columns:\n",
    "        * bill_length_mm\n",
    "        * bill_depth_mm\n",
    "        * flipper_length_mm\n",
    "        * body_mass_g\n",
    "    - Computes unique value counts for categorical columns:\n",
    "        * island\n",
    "        * sex\n",
    "    - Enables Evidently AI automated tests via `include_tests=True`\n",
    "    - Compares inference data against training data\n",
    "    - Returns the executed report results to the caller\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    train_data, infer_data = create_datasets()\n",
    "    eval_result = column_report_with_tests(train_data, infer_data)\n",
    "    # Inspect metrics and test outcomes in eval_result\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Report : Evidently AI report class for generating analyses\n",
    "    MedianValue : Metric for computing median values of numeric columns\n",
    "    UniqueValueCount : Metric for counting unique values in a column\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    column_report = Report([\n",
    "        MedianValue(column=\"bill_length_mm\"),\n",
    "        MedianValue(column=\"bill_depth_mm\"),\n",
    "        MedianValue(column=\"flipper_length_mm\"),\n",
    "        MedianValue(column=\"body_mass_g\"),\n",
    "        UniqueValueCount(column=\"island\"),\n",
    "        UniqueValueCount(column=\"sex\"),\n",
    "    ],\n",
    "    include_tests=True)\n",
    "    \n",
    "    # Execute the evaluations\n",
    "    column_report = column_report.run(inference_dataset, training_dataset)\n",
    "    \n",
    "    return column_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797585a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_tests = column_report_with_tests(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "column_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_drift_report(ev_ref_data: Dataset, ev_curr_data: Dataset) -> Report:\n",
    "    \"\"\"\n",
    "    Run an Evidently AI data drift and classification report.\n",
    "\n",
    "    This function compares current inference data against reference data using\n",
    "    Evidently AI presets for data drift and classification performance. The\n",
    "    report is executed and the evaluation results are returned to the caller.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ev_ref_data : Dataset\n",
    "        The reference Evidently AI dataset used as the baseline for drift\n",
    "        detection. Typically contains historical or training data.\n",
    "    ev_curr_data : Dataset\n",
    "        The current Evidently AI dataset containing recent inference data\n",
    "        to be compared against the reference data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    EvaluationResult\n",
    "        The result of running the Evidently AI report, containing computed\n",
    "        drift metrics and classification statistics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses `DataDriftPreset` with a threshold of 0.1 (10%)\n",
    "    - Includes `ClassificationPreset` for classification-related metrics\n",
    "    - Compares current data against reference data to detect distribution shifts\n",
    "    - Logs the start of the report execution process\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    ref_data, curr_data = create_datasets()\n",
    "    eval_result = run_report(ref_data, curr_data)\n",
    "    # Access drift metrics from eval_result\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Report : Evidently AI report class for generating analyses\n",
    "    DataDriftPreset : Preset configuration for data drift detection\n",
    "    ClassificationPreset : Preset configuration for classification metrics\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Running report...\")\n",
    "    # Create a report with pre-built evaluation templates for drift\n",
    "    # and classifications tasks\n",
    "    drift_report = Report([\n",
    "    DataDriftPreset(\n",
    "    threshold=0.1,\n",
    "        ),\n",
    "    ClassificationPreset()]\n",
    "    )\n",
    "    # Execute the evaluations\n",
    "    my_eval = drift_report.run(ev_curr_data, ev_ref_data)\n",
    "    #Add the evaluation to the current project\n",
    "\n",
    "    return my_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_report = run_drift_report(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "drift_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_features = apply_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa970dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_traffic(drift_features, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48416ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_inference_data = retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data(drift_inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d470d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_training_dataset, evidently_inference_dataset = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b966e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af5208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db526a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_report = column_report(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "column_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = run_report(evidently_training_dataset, evidently_inference_dataset, ws, project)\n",
    "\n",
    "my_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = run_report_test(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "my_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_tests = column_report_with_tests(evidently_training_dataset, evidently_inference_dataset)\n",
    "\n",
    "column_tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidently_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
